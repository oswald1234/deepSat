import torch

## pixel counts for each class in training data 
classCounts= {
    'test': torch.tensor([  65500,   20298,   74155,  206209,  585361,  611219,  188047,  517527,
                      76795, 1172746,   71018,    7126,   63845,   66483,   21290,   17382,
                     297343,  245904, 2813866,     162, 2537092,       0,       0, 8691147,
                     196563,     371,  161568, 1344999], dtype=torch.int32),
    
    'train': torch.tensor([  543068,   109468,   593878,  1618205,  4264564,  4426749,  1632281, 4250996,
                       655855,  9156598,   616274,    64119,   465173,   741620,   156909,  194494,
                      2280640,  2114830, 25928304,     3413, 22069305,        0,        0, 72758492,  
                      1343616,    13540,  1575432, 10915233],dtype=torch.int32), 
    
    'val': torch.tensor([ 70421,   27362,   97876,  225329,  496769,  480172,  204907,  625060,
                          85348, 1183352,   81149,    5152,   85187,   94839,   28321,   25553,
                         263204,  328436, 3367806,       0, 2721990,       0,       0, 9051679,
                         191556,    1516,  238360, 1383392], dtype=torch.int32)}

# 98% equal tail intervall "high" and "low" quantiles for training data for timeperiod 1 and 2 
quantiles={
    'high': {
        '1': torch.tensor([2144.0,1736.0,1406.0, 4732.0, 2482.31103515625, 3706.34228515625, 4520.2119140625, 4858.82177734375, 3850.68896484375, 
                           3037.412353515625]),
        '2': torch.tensor([1608.0, 1476.0, 1530.0, 4840.0, 2097.86767578125, 3959.4755859375, 4586.68798828125, 4826.0537109375, 2895.287109375, 
                           2207.1416015625])},
    'low': {
        '1': torch.tensor([100.0, 157.0, 108.0, 75.0, 104.94169616699219, 76.99676513671875, 84.23336791992188, 68.29641723632812, 48.8712272644043, 
                           35.631874084472656]),
        '2': torch.tensor([22.0, 40.0, 1.0, 1.0, 27.11801528930664, 4.137123107910156, 5.041072845458984, 4.071794509887695, 11.38535213470459, 
                           14.030078887939453])}
}

#old with added weight on "other roads and..."
ce_weights = torch.tensor([0.0000e+00, 5.1362e+02, 1.1472e+02, 4.4708e+01, 1.7092e+01, 1.6746e+01,
                            4.4391e+01, 1.6548e+01, 1.1023e+02, 1.0783e+02, 1.1555e+02, 1.0042e+03,
                            1.6943e+02, 9.5672e+01, 4.4588e+02, 3.8277e+02, 3.2361e+01, 3.2498e+01,
                            2.8015e+00, 2.0817e+04, 3.2352e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,
                            4.9671e+01, 5.8634e+03, 4.5644e+01, 6.7183e+00])


#old weights
#ce_weights =torch.tensor([
#    0.0000e+00, 5.1362e+02, 1.1472e+02, 4.4708e+01, 1.7092e+01, 1.6746e+01,
#    4.4391e+01, 1.6548e+01, 1.1023e+02, 7.8302e+00, 1.1555e+02, 1.0042e+03,
#    1.6943e+02, 9.5672e+01, 4.4588e+02, 3.8277e+02, 3.2361e+01, 3.2498e+01,
#    2.8015e+00, 2.0817e+04, 3.2352e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,
#    4.9671e+01, 5.8634e+03, 4.5644e+01, 6.7183e+00])