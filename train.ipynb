{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f876a90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only for testing\n",
    "# use main.py for training\n",
    "\n",
    "#To load a saved version of the model:\n",
    "#saved_model = UNET()\n",
    "#saved_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#TODO:\n",
    "# run on GPU\n",
    "\n",
    "# things to improve performance (speed)\n",
    "# use torch.utils.random_split https://www.programcreek.com/python/example/125046/torch.utils.data.random_split\n",
    "# https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html?highlight=checkpoint\n",
    "# implement \"channels Last Memory Format\" (beta) https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947076a9-f676-42ad-b111-6283eb3398c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded7b68-76fb-437e-9fb0-4a9d8466096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import munch\n",
    "import yaml\n",
    "\n",
    "def get_config(file='config.yaml'):\n",
    "    p = argparse.ArgumentParser(description='Learned Step Size Quantization')\n",
    "    p.add_argument('config_file', metavar='PATH', nargs='+',\n",
    "                   help='path to a configuration file')\n",
    "    arg = p.parse_args()\n",
    "\n",
    "    with open(file) as yaml_file:\n",
    "        cfg = yaml.safe_load(yaml_file)\n",
    "\n",
    "    for f in arg.config_file:\n",
    "        if not os.path.isfile(f):\n",
    "            raise FileNotFoundError('Cannot find a configuration file at', f)\n",
    "        with open(f) as yaml_file:\n",
    "            c = yaml.safe_load(yaml_file)\n",
    "            cfg = merge_nested_dict(cfg, c)\n",
    "\n",
    "    return munch.munchify(cfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99a7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0.dev20220425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x7f3538b75f70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure tensorboard is working\n",
    "#!conda list | grep tensorboard\n",
    "import torch\n",
    "#!conda install tensorboard -y\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(torch.__version__)\n",
    "\n",
    "#make sure this executes\n",
    "SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5339424f-02eb-423b-8ba5-43c4da0c3558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 16\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import munch\n",
    "\n",
    "def get_conf(path='config.yaml'):\n",
    "    with open(path) as file:\n",
    "        try:\n",
    "            return(munch.munchify(yaml.safe_load(file)))\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "        \n",
    "cfg = open_yaml()\n",
    "\n",
    "for kwarg in cfg.dataset.test.kwargs:\n",
    "    print(kwarg,cfg.dataset.test.kwargs[kwarg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1343bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exjobbare/software/miniconda3/envs/deepsat/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 2674 instances\n",
      "Validation set has 316 instances\n"
     ]
    }
   ],
   "source": [
    "# root to training data \n",
    "root = 'processed-data/TCI_256_split/Tidsperiod-1/train'\n",
    "rootv = 'processed-data/TCI_256_split/Tidsperiod-1/val'\n",
    "\n",
    "num_workers = 0\n",
    "GPU = False\n",
    "train = True\n",
    "\n",
    "batch_size=4\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "from dataset.datasets import sentinel\n",
    "from model.models import UNET\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from train import train,test \n",
    "\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Create datasets for training & validation,\n",
    "training_set= sentinel(root_dir=root,img_transform=transform)\n",
    "validation_set= sentinel(root_dir=rootv,img_transform=transform)\n",
    "\n",
    "if GPU:\n",
    "    pin_memmory = True\n",
    "else: \n",
    "    pin_memory = False \n",
    "    \n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "model = UNET()\n",
    "\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))\n",
    "\n",
    "# num_workers>0 enabels asynchronus dataloading and augmentation \n",
    "# num_workers should be tuned depending on the workload, CPU, GPU, and location of training data\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# use torch.no_grad() to disable gradients in validation and inference (less memory usage) \n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr = 0.001)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "030f47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise images in tensorboard\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "# https://pytorch.org/vision/main/auto_examples/plot_visualization_utils.html#sphx-glr-auto-examples-plot-visualization-utils-py\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "#show(img_grid)\n",
    "#matplotlib_imshow(img_grid, one_channel=False)\n",
    "#print('  '.join(classes[labels[j]] for j in range(4))\n",
    "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
    "# torch.utils.tensorboard.SummaryWriter is imported above\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/visualise_data')\n",
    "\n",
    "# Write image data to TensorBoard log dir\n",
    "writer.add_image('Four Sat Images', img_grid)\n",
    "writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#  tensorboard --logdir=runs\n",
    "# ...and open a browser tab to http://localhost:6006/\n",
    "\n",
    "#if on remote\n",
    "## on remote\n",
    "##tensorboard --logdir <path> --port 6006\n",
    "## forward everything on port 6006 on server to \n",
    "##ssh -L 16006:127.0.0.1:6006 -i vmexjobb_key.pem exjobbare@vdexjobb.westeurope.cloudapp.azure.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0752efb8-c0e9-47e3-9ab7-d52ca057bff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6cafe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train() missing 6 required positional arguments: 'device', 'train_loader', 'optimizer', 'loss_fn', 'epoch', and 'tb_writer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#make sure gradient tracking is on, and do one pass over the data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# turn of gradient tracking for reporting\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 6 required positional arguments: 'device', 'train_loader', 'optimizer', 'loss_fn', 'epoch', and 'tb_writer'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/s2_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    #make sure gradient tracking is on, and do one pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train(epoch_number,writer)\n",
    "    \n",
    "    # turn of gradient tracking for reporting\n",
    "    model.train(False)\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs,vlabels)\n",
    "        running_vloss += vloss\n",
    "        \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    # (tensor board) Log the running loss averaged per batch \n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                      {'Training': avg_loss, 'Validation': avg_vloss},\n",
    "                      epoch_number + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    # track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss: \n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(),model_path)\n",
    "    \n",
    "    epoch_number += 1\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8280c4da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m####################\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalidation_loader\u001b[49m:\n\u001b[1;32m     34\u001b[0m         X,y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     35\u001b[0m         preds \u001b[38;5;241m=\u001b[39m model(X)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_loader' is not defined"
     ]
    }
   ],
   "source": [
    "### see prediction vs label (should load unseen data before)\n",
    "### help functions\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "\n",
    "def plot_pred_lbl_rgb(pred,labl,rgb):\n",
    "    \n",
    "    # def fig\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    rows = 1\n",
    "    columns = 3\n",
    "    \n",
    "    # Adds a subplot at the 1st position\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(pred)\n",
    "    \n",
    "    plt.title(\"Pred (%d classes)\"%(len(torch.unique(pred)))) \n",
    "    \n",
    "    # Adds a subplot at the 2nd position\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(labl)          \n",
    "    plt.title(\"Label (%d classes)\"%(len(torch.unique(labl))))\n",
    "    \n",
    "    # Adds a subplot at the 3rd position\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(rgb.permute(1,2,0).numpy())\n",
    "    plt.title(\"RGB\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "####################\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        X,y = batch\n",
    "        preds = model(X)\n",
    "        preds = nn.functional.softmax(preds,dim=1)\n",
    "        predlabls = torch.argmax(preds,dim=1)\n",
    "        \n",
    "        for idx, predlabl in enumerate(predlabls):\n",
    "            plot_pred_lbl_rgb(predlabl,y[idx],X[idx])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "715a5e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/oskar/ESSD/deepSat/model/models.py:56: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape != concat_layer.shape:\n"
     ]
    }
   ],
   "source": [
    "#visualizing model in tensorboard\n",
    "\n",
    "\n",
    "# Again, grab a single mini-batch of images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# add_graph() will trace the sample input through your model,\n",
    "# and render it as a graph.\n",
    "writer.add_graph(model, images)\n",
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a772cf3982d27f7efa5dd5ee7ce70eb2de69f36da9772dbcefc10fee67f331e"
  },
  "kernelspec": {
   "display_name": "Python [conda env:deepsat]",
   "language": "python",
   "name": "conda-env-deepsat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
