{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c63b82-d7aa-4568-9342-95b502cbc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate quantiles for dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from dataset.datasets import s2stats, sentinel\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def my_quant(tens,hi=0.98,lo=0.02):\n",
    "    \n",
    "    def get_indices(q):\n",
    "        if int(q)<q:\n",
    "            ind = [int(q),int(q+1)]\n",
    "        else:\n",
    "            ind=[int(q)]\n",
    "        return(ind)\n",
    "    \n",
    "    def calc_quant(ind,sort):\n",
    "        if len(ind) >1:\n",
    "            q = (sort[ind[0]]+sort[ind[1]])/2    \n",
    "        else:\n",
    "            q=sort[ind]\n",
    "        return(q.numpy().item()) \n",
    "            \n",
    "    \n",
    "    sort,_ = torch.sort(tens)\n",
    "   \n",
    "    n = tens.shape[0]-1\n",
    "    ind_hi = get_indices(hi*(n))\n",
    "    ind_lo = get_indices(lo*(n))\n",
    "    return(calc_quant(ind_lo,sort),calc_quant(ind_hi,sort))\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "quanthi=[]\n",
    "quantlo=[]\n",
    "median_bands=[]\n",
    "dset= s2stats(root_dir='processed-data/dsen_2_256_new_split/timeperiod1/train/')\n",
    "\n",
    "for band in tqdm(range(10)):\n",
    "    for id,(img,_) in enumerate(tqdm(dset)):\n",
    "        if id==0:\n",
    "            conc=img[band,:,:].view(-1)\n",
    "        else:\n",
    "            conc=torch.cat((conc,img[band,:,:].view(-1)),0)\n",
    "        \n",
    "    q = my_quant(conc)\n",
    "    quantlo.append(q[0])\n",
    "    quanthi.append(q[1])\n",
    "    median_bands.append(conc.median().item())\n",
    "\n",
    "print(\"q_hi =\",quanthi)\n",
    "print('q_lo =',quantlo)\n",
    "print('median =', median_bands)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef964cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classCounts (count how many pixels of each class).\n",
    "\n",
    "from dataset.utils import classCount,pNormalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from dataset.datasets import sentinel\n",
    "# Create experimental dataset, rgb=True for 3 channels (default = False)\n",
    "# POINT TO FOLDER WITH TIMEPERIOD(S) WITH SUBFOLDERS: 'test, 'train, 'val\n",
    "q_hi = torch.tensor([2102.0, 1716.0, 1398.0, 4732.0, 2434.42919921875, 3701.759765625, 4519.2177734375, 4857.7734375, 3799.80322265625, 3008.8935546875])\n",
    "q_lo = torch.tensor([102.0, 159.0, 107.0, 77.0, 106.98081970214844, 79.00384521484375, 86.18966674804688, 70.40167236328125, 50.571197509765625, 36.95356750488281])    \n",
    "norm = pNormalize(maxPer=q_hi,minPer=q_lo)\n",
    "\n",
    "def get_set_classcounts(timeperiod=1):\n",
    "    BATCH_SIZE=10\n",
    "    NUM_WORKERS = 2 \n",
    "    test_set = sentinel(root_dir='processed-data/dsen_2_256_new_split/', img_transform=norm,data=\"test\",timeperiod=timeperiod)\n",
    "    train_set=sentinel(root_dir='processed-data/dsen_2_256_new_split/', img_transform=norm,data=\"train\",timeperiod=timeperiod)\n",
    "    val_set=sentinel(root_dir='processed-data/dsen_2_256_new_split/', img_transform=norm,data=\"val\",timeperiod=timeperiod)\n",
    "    \n",
    "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    test_classCounts,_ = classCount(test_loader)\n",
    "    train_classCounts,_ = classCount(train_loader)\n",
    "    val_classCounts,_ = classCount(val_loader)\n",
    "    \n",
    "    return(test_classCounts,train_classCounts,val_classCounts)\n",
    "\n",
    "test_cc1,train_cc1,val_cc1 = get_set_classcounts(timeperiod=1)\n",
    "test_cc2,train_cc2,val_cc2 = get_set_classcounts(timeperiod=2)\n",
    "\n",
    "classCounts= {\n",
    "    'test':{\n",
    "        '1':test_cc1,\n",
    "        '2':test_cc2},\n",
    "    'train':{\n",
    "        '1':train_cc1,\n",
    "        '2':train_cc2},\n",
    "    'val':{\n",
    "        '1':val_cc1,\n",
    "        '2':val_cc2}\n",
    "}\n",
    "\n",
    "classCounts1= {\n",
    "    'test':test_cc1,\n",
    "    'train':train_cc1,\n",
    "    'val':val_cc1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d086b04-69b6-4e55-be7d-71df606d474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### calculate std and mean for dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.datasets import s2stats\n",
    "import torch\n",
    "import numpy as np\n",
    "dset= s2stats(root_dir='processed-data/dsen_2_256_new_split/timeperiod1/train/')\n",
    "\n",
    "loader = DataLoader(dset,\n",
    "                         batch_size=1,\n",
    "                         num_workers=0,\n",
    "                         shuffle=False)\n",
    "\n",
    "loader = tqdm(loader)\n",
    "mean = 0.\n",
    "var = 0.\n",
    "ninstance=0.\n",
    "for images in loader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    \n",
    "    m=images.mean(2).sum(0)\n",
    "    v=images.var(2).sum(0)\n",
    "   \n",
    "    if not torch.any(torch.isinf(v)):\n",
    "        mean += m\n",
    "        var += v\n",
    "        ninstance += batch_samples\n",
    "\n",
    "mean /= ninstance\n",
    "var /= ninstance\n",
    "std = torch.sqrt(var)\n",
    "\n",
    "print('std = ',std.numpy())\n",
    "print('mean =',mean.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f66896",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine weights \n",
    "\n",
    "import torch\n",
    "from preprocess.classDict import class_dict\n",
    "from dataset.stats import ce_weights,classCounts\n",
    "counts= classCounts['train']\n",
    "\n",
    "n_samples = torch.sum(counts[1:28])\n",
    "n_classes = counts.shape[0]-3\n",
    "\n",
    "weights  = n_samples/(n_classes*counts)\n",
    "weights[torch.where(counts==0)] = 0\n",
    "#torch.max(weights)\n",
    "#torch.min(weights[torch.where(weights!=0)])\n",
    "#torch.max(ce_weights)\n",
    "#torch.min(ce_weights[torch.where(ce_weights!=0)])\n",
    "#weights[9] += 100\n",
    "#weights[18]+=10\n",
    "#weights[20] +=10\n",
    "#weights[23] +=10\n",
    "#weights[27] +=1\n",
    "\n",
    "for ii, v in enumerate(class_dict.values()):\n",
    "    i=ii+1\n",
    "    print(i,round(ce_weights[i].item(),2),' ',round(weights[i].item(),2),' ',v['class_name'],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbc833-9265-4b8b-a09b-2ec50f7e64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  some functions\n",
    "import torch\n",
    "\n",
    "#------------quant_batch----------------------\n",
    "# takes in tensor of shape [batchsize,band,H,w]\n",
    "# max batchsize is 256\n",
    "# permutes dimensions to [band,batchsize,H,W]\n",
    "# flattens dimensions to [band,batchsize*H*W]\n",
    "# calculates quantile for each band on dim=1\n",
    "\n",
    "def quant_batch(imbatch,q=torch.tensor([0.02,0.98])):\n",
    "    perm = imbatch.permute(1,0,2,3).flatten(start_dim=1)\n",
    "    q = torch.quantile(perm, q, dim=1,interpolation='midpoint')\n",
    "    return(q)\n",
    "\n",
    "#------example use case\n",
    "# define dataset and dataloader, \n",
    "# dset= s2stats(root_dir='processed-data/dsen_2_256_split/*/*/')\n",
    "# dloader = DataLoader(dset, batch_size=(100), num_workers=0,pin_memory=True,shuffle=True)\n",
    "#\n",
    "# dataiter = iter(dloader)\n",
    "# imgs = dataiter.next()\n",
    "#  \n",
    "# qbatch = quant_batch(imgs.float())\n",
    "# qbatch.shape\n",
    "# output: torch.Size([2,10]) eg  index [0,1] = lower quantile for band 1\n",
    "\n",
    "\n",
    "#----------mean_batch--------\n",
    "# takes in tensor of shape [batchsize,band,H,w]\n",
    "# replaces all inf values with nan to prevent overflow\n",
    "# calculates mean for each band\n",
    "# returns mean for each band: torch.Size[channels]\n",
    "def mean_batch(batch):\n",
    "    b=batch.nan_to_num(nan=torch.nan, posinf=torch.nan, neginf=torch.nan)\n",
    "    return(torch.nanmean(batch,dim=(0,2,3),out=torch.empty(batch.shape[1])))\n",
    "#-----------std_batch----------\n",
    "# takes in tensor of shape [batchsize,band,H,w]\n",
    "# replaces all inf values with nan to prevent overflow\n",
    "# convert to numpy array\n",
    "# calculates std for each band\n",
    "# returns std for each band: torch.Size[channels]\n",
    "def std_batch(batch):\n",
    "    b=batch.nan_to_num(nan=torch.nan,posinf=torch.nan,neginf=torch.nan).numpy()\n",
    "    std =np.nanstd(b,axis=(0,2,3),out=np.empty(batch.shape[1]))\n",
    "    return(torch.from_numpy(std))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307aa03-52ae-4ead-a809-d9a10ca2dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check inf values\n",
    "\n",
    "import torch\n",
    "from dataset.datasets import s2stats, sentinel\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#stat= s2stats(root_dir='processed-data/dsen_2_256_new_split/timeperiod1/train/')\n",
    "sent= sentinel(root_dir='processed-data/dsen_2_256_new_split/timeperiod1/train/')\n",
    "\n",
    "def isinf(img):\n",
    "    return(torch.any(torch.isinf(img)))\n",
    "\n",
    "statinf=[]\n",
    "sentinf=[]\n",
    "for ind,(img,_) in enumerate(tqdm(sent)):\n",
    "    if isinf(img):\n",
    "        sentinf.append(ind)\n",
    "        \n",
    "       \n",
    "print('inf img from sentinel:\\n',len(sentinf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ae743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine optuna parameters \n",
    "\n",
    "from math import floor,log\n",
    "# reduction_factor = n\n",
    "red_factor=4\n",
    "# min_resource = r\n",
    "min_resource= 2 \n",
    "#max_resource/min_resource =  R\n",
    "max_resource = 50\n",
    "\n",
    "# min_early_stopping_rate = s\n",
    "min_early_stopping_rate = 1\n",
    "\n",
    "\n",
    "R = max_resource/min_resource\n",
    "\n",
    "B = floor(log(max_resource))*R +1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepsat]",
   "language": "python",
   "name": "conda-env-deepsat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
