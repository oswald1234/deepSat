{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240046b6",
   "metadata": {},
   "source": [
    "### Preprocess Raw-data\n",
    "\n",
    "start env preprocess\n",
    "\n",
    "#### import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ede923-fb96-4874-87ad-c4b3827cd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import h5netcdf\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rio\n",
    "from geopandas import read_file\n",
    "from tqdm.notebook import tqdm\n",
    "from preprocess.uaUtils import open_fua\n",
    "from preprocess.s2Utils import search_product,open_tile,clip_tile, get_tile_info, get_prod_info\n",
    "from preprocess.utils import rasterize,get_obj_within,str_to_oao, save_cube,class_dict,org_files\n",
    "long_name=('B4 (665 nm)',\n",
    " 'B3 (560 nm)',\n",
    " 'B2 (490 nm)',\n",
    " 'B8 (842 nm)',\n",
    " 'SRB5 (705 nm)',\n",
    " 'SRB6 (740 nm)',\n",
    " 'SRB7 (783 nm)',\n",
    " 'SRB8A (865 nm)',\n",
    " 'SRB11 (1610 nm)',\n",
    " 'SRB12 (2190 nm)') \n",
    "xr.set_options(keep_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503c6db",
   "metadata": {},
   "source": [
    "### User input\n",
    "\n",
    "define where the raw-data is located (selected Sentinel tiles, patches/AOI, and LULC-Data)\n",
    "and define where to save the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f723f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Define root_path containing: \n",
    "# \"raw-data/AOI/tile_selection.gpkg\", \n",
    "# 'raw-data/AOI/patch_size_features.gpkg', \n",
    "# 'raw-data/LULC-Sweden/*/Data/*.gpkg'\n",
    "\n",
    "root_path = os.getcwd()\n",
    "timeperiod = 1 # timeperiod \n",
    "patch_size = 256  #choose between ['64','128','256'] \n",
    "\n",
    "# Path to s2 products\n",
    "##s2_path = os.path.join(root_path,'raw-data/sentinel-2/timeperiod{}/'.format(timeperiod))\n",
    "s2_path = os.path.join(root_path,'../dsen2_results/timeperiod{}/'.format(timeperiod))\n",
    "\n",
    "# Path to Urban Atlas (UA) data\n",
    "ua_paths = glob.glob(os.path.join(root_path,'raw-data/LULC-Sweden/*/Data/*.gpkg'))\n",
    "\n",
    "# Define where to save processed data\n",
    "source = 'dsen2'\n",
    "data_source = source + '_{}_new'.format(patch_size)\n",
    "sdir = os.path.join(root_path,'processed-data/{source}/timeperiod{tp}'.format(source=data_source,tp=timeperiod))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81686f71",
   "metadata": {},
   "source": [
    "#### Check if products exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Looking for sentinel products in:\\n',s2_path,'\\n')\n",
    "\n",
    "# get crs\n",
    "ua_crs = read_file(ua_paths[0]).crs\n",
    "\n",
    "# path to s2 tiles to use \n",
    "s2tiles = read_file(os.path.join(root_path,'raw-data/AOI/tile_selection.gpkg')).to_crs(ua_crs)\n",
    "\n",
    "# path to scb patches of size*size\n",
    "scb_grid = read_file(os.path.join(root_path,'raw-data/AOI/patch_size_features.gpkg'),layer = \"patch_{}\".format(patch_size)).to_crs(ua_crs)\n",
    "\n",
    "\n",
    "# check that desired s2tiles existst in s2_path\n",
    "print('---- Found sentinel products: ----')\n",
    "for tile in s2tiles.itertuples():\n",
    "    print(tile.Name)\n",
    "    prod,date= search_product(s2_path,tile.Name)\n",
    "    if prod:\n",
    "        for i, prod in enumerate(prod):\n",
    "            fn=os.path.basename(prod)\n",
    "            print('Found:',get_prod_info(fn)['Tile'],pd.to_datetime(date[i]),fn)\n",
    "    else:\n",
    "        print('No found sentinel products for tile:',tile.Name)\n",
    "          \n",
    "# list found FUAS\n",
    "print('\\n---- Found LULC data: ----')\n",
    "if ua_paths:\n",
    "    for ua_path in ua_paths:\n",
    "        print('Found:',os.path.basename(ua_path))\n",
    "else:\n",
    "    print('No LULC data found')\n",
    "    \n",
    "\n",
    "print('\\n Save processed data to: \\n',  sdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a891104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential all fuas (original)\n",
    "bad_patch=[]\n",
    "for path in tqdm(ua_paths, desc='Total progress'):\n",
    "    \n",
    "    fua_labls,fua_bound = open_fua(path)             # open fua layers\n",
    "    tiles_inters = s2tiles.sjoin(fua_bound, predicate='intersects')  #get intersecting tiles for fua\n",
    "    fua_name = str_to_oao(fua_bound.fua_name[0])\n",
    "    \n",
    "    for tile in tqdm(tiles_inters.itertuples(),desc='Intersecting tiles',total=len(tiles_inters)): # for every intersecting tile\n",
    "        \n",
    "        # select patches within curr tile\n",
    "        curr_tile = s2tiles[s2tiles.Name.isin([tile.Name])]                                               \n",
    "        patches_within = get_obj_within(get_obj_within(scb_grid,curr_tile),fua_bound) #select patches(grid) within curr_tile and within fua\n",
    "        #savedir\n",
    "        savedir = os.path.join(sdir,'{}/{}/'.format(fua_name,tile.Name))\n",
    "        #print(savedir)\n",
    "        #open curr tile \n",
    "        s2_tile = open_tile(tile.Name,s2_path)\n",
    "    \n",
    "        for patch in tqdm(patches_within.itertuples(),total=len(patches_within), desc='FUA: {}, Tile: {}'.format(fua_name,tile.Name)):\n",
    "            \n",
    "            # get current patch and clip labels to patch extent\n",
    "            curr_patch = scb_grid[(scb_grid.id.isin([patch.id]))]\n",
    "            # clip tile and labels to patch extent\n",
    "            s2_patch = clip_tile(s2_tile,curr_patch)\n",
    "            s2_patch = s2_patch.astype(np.float32)\n",
    "            patch_labls = fua_labls.clip(curr_patch)\n",
    "            #rasterize patch labels \n",
    "            try:\n",
    "                cube = rasterize(patch_labls)\n",
    "            except:\n",
    "                bad_patch.append(patch.id)\n",
    "            else:    \n",
    "                #merge s2_patch and patch labels\n",
    "                cube = cube.merge(s2_patch.rio.reproject_match(cube)) #append to cube\n",
    "                # reduce coordinate dimensions by one\n",
    "                if cube.dims['x'] > patch_size:\n",
    "                    cube=cube.isel(x=slice(None, -1), y=slice(None, -1))           \n",
    "            \n",
    "                #set attributes to cube before saving\n",
    "                cube.attrs[\"patch_id\"] = patch.id\n",
    "                cube.attrs['FUA'] = fua_name\n",
    "                cube.attrs['long_name']=long_name\n",
    "                for key,value in get_tile_info(tile.Name,s2_path).items():\n",
    "                    cube.attrs[key] = value\n",
    "                #cube.train_id.attrs['_fillValue']=255\n",
    "                #cube.class_code.attrs['_fillValue']=255\n",
    "                save_cube(cube,savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d3afc",
   "metadata": {},
   "source": [
    "### Remove bad patches \n",
    "Execute this step for once for each timeperiod. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d50b6d-447a-4e18-b1fa-be378919f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate min max to check for bad data\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from dataset.datasets import s2stats, sentinel\n",
    "#import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dset= s2stats(root_dir='processed-data/dsen2_256_new/timeperiod1/*/*')\n",
    "#dset= sentinel(root_dir='processed-data/dsen2_256_new_split/',timeperiod=2)\n",
    "print(len(dset))\n",
    "max_vals=np.zeros((len(dset),10))\n",
    "min_vals=np.zeros((len(dset),10))\n",
    "\n",
    "count = 0\n",
    "paths = []\n",
    "\n",
    "for idx, (img,path) in enumerate(dset):\n",
    "   \n",
    "    maxs = img.numpy().max(axis=(1,2))\n",
    "    mins = img.numpy().min(axis=(1,2))\n",
    "    \n",
    "    if any(maxs >3.4e+38):\n",
    "        count +=1\n",
    "        paths.append(path)\n",
    "    \n",
    "    max_vals[idx,:]=img.numpy().max(axis=(1,2))\n",
    "    min_vals[idx,:]=img.numpy().min(axis=(1,2))\n",
    "    \n",
    "maxs= max_vals.max(axis=0)\n",
    "mins= min_vals.min(axis=0)\n",
    "\n",
    "#print(maxs)\n",
    "#print(mins)\n",
    "\n",
    "\n",
    "for path in paths: \n",
    "    os.remove(path)\n",
    "\n",
    "print('{} bad patches are removed.'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29143a4",
   "metadata": {},
   "source": [
    "### split train/test/val\n",
    "\n",
    "when AOI/patches for timeperiod 1 and 2 are preprocessed \n",
    "\n",
    "1. Split dataset (if satisfied, execute step 2).\n",
    "2. Reorganize files after split.\n",
    "\n",
    "#### step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find_tp2_patches()\n",
    "# takes in a list of paths to patches in timeperiod 1, \n",
    "# finds and returns corresponding patch by patch id in timeperiod_2\n",
    "def find_tp2_patches(df):\n",
    "    df_2 = []\n",
    "    for path in df:\n",
    "        patch = os.path.split(path)[-1].split('_')\n",
    "        patch_id='_'.join(patch[0:3])\n",
    "        df_2.append(glob.glob(os.path.join('processed-data/dsen_2_256_new/timeperiod2/*/*',patch_id+'_*'))[0])\n",
    "    return(df_2)\n",
    "\n",
    "#1 split dataset\n",
    "import glob\n",
    "import os\n",
    "train = 0.8\n",
    "test = 0.5 #of remaining -train\n",
    "sdir='/home/exjobbare/projects/datadrive0/deepSat/processed-data/dsen_2_256_new/timeperiod1/'\n",
    "df = np.asarray(glob.glob(os.path.join(sdir,'*/*/*.nc')))\n",
    "total = len(df)\n",
    "msk = np.random.rand(len(df)) <train\n",
    "len(msk[msk==True])/len(msk)\n",
    "\n",
    "train = df[msk]\n",
    "train_2 = find_tp2_patches(train)\n",
    "testval = df[~msk]\n",
    "msk = np.random.rand(len(testval))<test\n",
    "test=testval[msk]\n",
    "test_2=find_tp2_patches(test)\n",
    "val = testval[~msk]\n",
    "val_2=find_tp2_patches(val)\n",
    "\n",
    "print('total:',total,'train:', len(train) ,round(len(train)/len(df)*100,0),'test:',len(test),round(len(test)/len(df)*100,0),'val:',len(val),round(len(val)/len(df)*100,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889c483",
   "metadata": {},
   "source": [
    "#### step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d28b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2 copy files to 'source_split' '/train' '/test' or '/val'\n",
    "if train.size>0: \n",
    "    org_files(train,mode='train')\n",
    "    org_files(train_2,mode='train')\n",
    "if test.size>0:\n",
    "    org_files(test,mode='test')\n",
    "    org_files(test_2,mode='test')\n",
    "if val.size>0:\n",
    "    org_files(val,mode='val')\n",
    "    org_files(val_2,mode='val')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a7be5",
   "metadata": {},
   "source": [
    "#### Compact Naming Convention\n",
    "\n",
    "The compact naming convention is arranged as follows:\n",
    "\n",
    "MMM_MSIXXX_YYYYMMDDHHMMSS_Nxxyy_ROOO_Txxxxx_<Product Discriminator>.SAFE\n",
    "\n",
    "The products contain two dates.\n",
    "\n",
    "The first date (YYYYMMDDHHMMSS) is the datatake sensing time.\n",
    "The second date is the \"Product Discriminator\" field, which is 15 characters in length, and is used to distinguish between different end user products from the same datatake. Depending on the instance, the time in this field can be earlier or slightly later than the datatake sensing time.\n",
    "\n",
    "The other components of the filename are:\n",
    "\n",
    "* MMM: is the mission ID(S2A/S2B)\n",
    "* MSIXXX: MSIL1C denotes the Level-1C product level/ MSIL2A denotes the Level-2A product level\n",
    "* YYYYMMDDHHMMSS: the datatake sensing start time\n",
    "* Nxxyy: the PDGS Processing Baseline number (e.g. N0204)\n",
    "* ROOO: Relative Orbit number (R001 - R143)\n",
    "* Txxxxx: Tile Number field\n",
    "\n",
    "SAFE: Product Format (Standard Archive Format for Europe)\n",
    "\n",
    "Source https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/naming-convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8f094-540d-4f18-b5ef-6ff75f9c2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_p = ['46008',\n",
    " '48950',\n",
    " '47261',\n",
    " '45171',\n",
    " '45997',\n",
    " '45592',\n",
    " '47692',\n",
    " '41799',\n",
    " '48950',\n",
    " '48514',\n",
    " '48514',\n",
    " '60278',\n",
    " '60679',\n",
    " '56922',\n",
    " '53536',\n",
    " '56077',\n",
    " '58600',\n",
    " '59854',\n",
    " '59009',\n",
    " '56053',\n",
    " '51452',\n",
    " '9139',\n",
    " '8715',\n",
    " '7450',\n",
    " '4934',\n",
    " '5354',\n",
    " '4082',\n",
    " '5349',\n",
    " '4923',\n",
    " '11652',\n",
    " '8299',\n",
    " '9551',\n",
    " '10387',\n",
    " '8719',\n",
    " '7050',\n",
    " '9139',\n",
    " '8719',\n",
    " '7470',\n",
    " '8299',\n",
    " '60303',\n",
    " '64920',\n",
    " '56945',\n",
    " '62823',\n",
    " '59454',\n",
    " '59460',\n",
    " '57356',\n",
    " '65340',\n",
    " '60731',\n",
    " '60723',\n",
    " '61151',\n",
    " '59457',\n",
    " '60311',\n",
    " '54844',\n",
    " '59891',\n",
    " '62403',\n",
    " '61143',\n",
    " '59880',\n",
    " '59454',\n",
    " '59871',\n",
    " '58607',\n",
    " '63229',\n",
    " '63226',\n",
    " '57346',\n",
    " '60288',\n",
    " '63640',\n",
    " '62809',\n",
    " '59024',\n",
    " '16274',\n",
    " '15017',\n",
    " '15430',\n",
    " '13760',\n",
    " '15017',\n",
    " '45635',\n",
    " '42276',\n",
    " '42276',\n",
    " '47310',\n",
    " '45635',\n",
    " '15505',\n",
    " '16766',\n",
    " '15087',\n",
    " '26785',\n",
    " '26365',\n",
    " '28035',\n",
    " '28455',\n",
    " '28867',\n",
    " '28455',\n",
    " '28035',\n",
    " '33010',\n",
    " '31752',\n",
    " '37211',\n",
    " '35105',\n",
    " '34262',\n",
    " '38048',\n",
    " '41027',\n",
    " '40601',\n",
    " '43553',\n",
    " '41035',\n",
    " '38923',\n",
    " '35984',\n",
    " '40604',\n",
    " '39349',\n",
    " '18468',\n",
    " '19313',\n",
    " '20985',\n",
    " '18041',\n",
    " '17213',\n",
    " '17200',\n",
    " '19306',\n",
    " '19730',\n",
    " '18896']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:preprocess]",
   "language": "python",
   "name": "conda-env-preprocess-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a772cf3982d27f7efa5dd5ee7ce70eb2de69f36da9772dbcefc10fee67f331e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
