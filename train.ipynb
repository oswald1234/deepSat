{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f876a90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only for testing\n",
    "# use main.py for training\n",
    "\n",
    "\n",
    "# things to improve performance (speed)\n",
    "# use torch.utils.random_split https://www.programcreek.com/python/example/125046/torch.utils.data.random_split\n",
    "# https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html?highlight=checkpoint\n",
    "# implement \"channels Last Memory Format\" (beta) https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947076a9-f676-42ad-b111-6283eb3398c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2524\n",
      "output: torch.Size([10, 28, 256, 256])\n",
      "labl torch.Size([10, 256, 256])\n",
      "pred: torch.Size([10, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# sample image\n",
    "import torch\n",
    "\n",
    "from dataset.utils import pNormalize\n",
    "from dataset.datasets import sentinel\n",
    "from model.models import UNET\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "q_hi = torch.tensor([2102.0, 1716.0, 1398.0, 4732.0, 2434.42919921875, 3701.759765625, 4519.2177734375, 4857.7734375, 3799.80322265625, 3008.8935546875])\n",
    "q_lo = torch.tensor([102.0, 159.0, 107.0, 77.0, 106.98081970214844, 79.00384521484375, 86.18966674804688, 70.40167236328125, 50.571197509765625, 36.95356750488281])\n",
    "        \n",
    "norm = pNormalize(\n",
    "                maxPer=q_hi,\n",
    "                minPer=q_lo\n",
    ")\n",
    "\n",
    "\n",
    "# Create experimental dataset and loader, rgb=True for 3 channels (default = False)\n",
    "test_set = sentinel(root_dir='processed-data/dsen_2_256_new_split', img_transform=norm)\n",
    "test_loader = DataLoader(test_set, batch_size=10, num_workers=0)\n",
    "\n",
    "print(len(test_set))\n",
    "\n",
    "# access one image from dataset\n",
    "img, label = test_set[0]\n",
    "#print('image shape:', img.shape)\n",
    "\n",
    "\n",
    "# or  batch from DataLoader\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define model\n",
    "model = UNET(in_channels=images.shape[1])\n",
    "model.load_state_dict(torch.load('to/model_epoch_95.pt',map_location=torch.device('cpu')))\n",
    "# disable gradients etc. model.train() for training https://medium.com/jun94-devpblog/pytorch-6-model-train-vs-model-eval-no-grad-hyperparameter-tuning-3812c216a3bd\n",
    "model.eval()\n",
    "\n",
    "output = model(images)\n",
    "\n",
    "#pred = torch.argmax(output, dim=1)\n",
    "     # prediction\n",
    "pred= torch.nn.functional.softmax(output,dim=1)\n",
    "pred = torch.argmax(pred,dim=1)\n",
    "\n",
    "\n",
    "#print('prediction:', pred.shape)\n",
    "#print('labels:', labels.shape)\n",
    "\n",
    "print('output:',output.shape)\n",
    "print('labl', labels.shape)\n",
    "print('pred:',pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad8ac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "from train.metrics import plot_sample, plot_batch\n",
    "\n",
    "for idx,(images,labels) in enumerate(test_loader):\n",
    "#dataiter = iter(test_loader)\n",
    "#images, labels = dataiter.next()\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "        pred= torch.nn.functional.softmax(output,dim=1)\n",
    "        pred = torch.argmax(pred,dim=1)\n",
    "        plot_batch(pred,labels, images)\n",
    "        \n",
    "        for i in range(pred.shape[0]):\n",
    "            plot_sample(pred[i,:,:],labels[i,:,:], images[i,0:3,:,:]  )\n",
    "            break\n",
    "            #plot_pred_lbl_rgb(pred[i,:,:],labels[i,:,:], images[i,0:3,:,:]  )\n",
    "        if idx==0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020cb080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 513.62   47.68   Continuous Urban Fabric (S.L. &amp;gt; 80%) \n",
      "\n",
      "2 114.72   10.65   Discontinuous Dense Urban Fabric (S.L. : 50% - 80%) \n",
      "\n",
      "3 44.71   4.15   Discontinuous Medium Density Urban Fabric (S.L. : 30% - 50%) \n",
      "\n",
      "4 17.09   1.59   Discontinuous Low Density Urban Fabric (S.L. : 10% - 30%) \n",
      "\n",
      "5 16.75   1.55   Discontinuous Very Low Density Urban Fabric (S.L. &amp;lt; 10%) \n",
      "\n",
      "6 44.39   4.12   Isolated Structures \n",
      "\n",
      "7 16.55   1.54   Industrial, commercial, public, military and private units \n",
      "\n",
      "8 110.23   10.23   Fast transit roads and associated land \n",
      "\n",
      "9 107.83   100.73   Other roads and associated land \n",
      "\n",
      "10 115.55   10.73   Railways and associated land \n",
      "\n",
      "11 1004.2   93.22   Port areas \n",
      "\n",
      "12 169.43   15.73   Airports \n",
      "\n",
      "13 95.67   8.88   Mineral extraction and dump sites \n",
      "\n",
      "14 445.88   41.39   Construction sites \n",
      "\n",
      "15 382.77   35.53   Land without current use \n",
      "\n",
      "16 32.36   3.0   Green urban areas \n",
      "\n",
      "17 32.5   3.02   Sports and leisure facilities \n",
      "\n",
      "18 2.8   10.26   Arable land (annual crops) \n",
      "\n",
      "19 20817.0   1932.35   Permanent crops (vineyards, fruit trees, olive groves) \n",
      "\n",
      "20 3.24   10.3   Pastures \n",
      "\n",
      "21 0.0   0.0   Complex and mixed cultivation patterns \n",
      "\n",
      "22 0.0   0.0   Orchards at the fringe of urban classes \n",
      "\n",
      "23 1.0   10.09   Forests \n",
      "\n",
      "24 49.67   4.61   Herbaceous vegetation associations (natural grassland, moors...) \n",
      "\n",
      "25 5863.4   544.28   Open spaces with little or no vegetations (beaches, dunes, bare rocks, glaciers) \n",
      "\n",
      "26 45.64   4.24   Wetland \n",
      "\n",
      "27 6.72   1.62   Water bodies \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 5.1362e+02, 1.1472e+02, 4.4708e+01, 1.7092e+01, 1.6746e+01,\n",
       "        4.4391e+01, 1.6548e+01, 1.1023e+02, 1.0783e+02, 1.1555e+02, 1.0042e+03,\n",
       "        1.6943e+02, 9.5672e+01, 4.4588e+02, 3.8277e+02, 3.2361e+01, 3.2498e+01,\n",
       "        2.8015e+00, 2.0817e+04, 3.2352e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        4.9671e+01, 5.8634e+03, 4.5644e+01, 6.7183e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from preprocess.classDict import class_dict\n",
    "counts= torch.tensor([  \n",
    "    535419,   138327,   619285,  1589119,  4156781,  4242609, 1600499, 4293367,\n",
    "    644562,  9073507,   614877,    70747,   419341,  742613,  159342,   185612,\n",
    "    2195461,  2186212, 25360177,     3413, 21960385, 0,       0,      71047030,\n",
    "    1430349,    12117,  1556533, 10575180],\n",
    "    dtype=torch.int32)\n",
    "\n",
    "ce_weights =torch.tensor([\n",
    "    0.0000e+00, 5.1362e+02, 1.1472e+02, 4.4708e+01, 1.7092e+01, 1.6746e+01,\n",
    "    4.4391e+01, 1.6548e+01, 1.1023e+02, 7.8302e+00, 1.1555e+02, 1.0042e+03,\n",
    "    1.6943e+02, 9.5672e+01, 4.4588e+02, 3.8277e+02, 3.2361e+01, 3.2498e+01,\n",
    "    2.8015e+00, 2.0817e+04, 3.2352e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
    "    4.9671e+01, 5.8634e+03, 4.5644e+01, 6.7183e+00])\n",
    "\n",
    "n_samples = torch.sum(counts[1:28])\n",
    "n_classes = counts.shape[0]-3\n",
    "\n",
    "weights  = n_samples/(n_classes*counts)\n",
    "weights[torch.where(counts==0)] = 0\n",
    "#torch.max(weights)\n",
    "#torch.min(weights[torch.where(weights!=0)])\n",
    "#torch.max(ce_weights)\n",
    "#torch.min(ce_weights[torch.where(ce_weights!=0)])\n",
    "weights[9] += 100\n",
    "ce_weights[9] +=100\n",
    "weights[18]+=10\n",
    "weights[20] +=10\n",
    "weights[23] +=10\n",
    "weights[27] +=1\n",
    "\n",
    "for ii, v in enumerate(class_dict.values()):\n",
    "    i=ii+1\n",
    "    print(i,round(ce_weights[i].item(),2),' ',round(weights[i].item(),2),' ',v['class_name'],'\\n')\n",
    "\n",
    "weights[9]\n",
    "\n",
    "ce_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0.dev20220425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x7f3538b75f70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this to make sure tensorboard is working\n",
    "#!conda list | grep tensorboard\n",
    "import torch\n",
    "#!conda install tensorboard -y\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(torch.__version__)\n",
    "\n",
    "#make sure this executes\n",
    "SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030f47c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/oskar/ESSD/deepSat/train.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/oskar/ESSD/deepSat/train.ipynb#ch0000001?line=0'>1</a>\u001b[0m \u001b[39m#visualise images in tensorboard\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/oskar/ESSD/deepSat/train.ipynb#ch0000001?line=2'>3</a>\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(training_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/oskar/ESSD/deepSat/train.ipynb#ch0000001?line=3'>4</a>\u001b[0m images, labels \u001b[39m=\u001b[39m dataiter\u001b[39m.\u001b[39mnext()\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/oskar/ESSD/deepSat/train.ipynb#ch0000001?line=5'>6</a>\u001b[0m \u001b[39m# Create a grid from the images and show them\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/oskar/ESSD/deepSat/train.ipynb#ch0000001?line=6'>7</a>\u001b[0m \u001b[39m# https://pytorch.org/vision/main/auto_examples/plot_visualization_utils.html#sphx-glr-auto-examples-plot-visualization-utils-py\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_loader' is not defined"
     ]
    }
   ],
   "source": [
    "#visualise images in tensorboard\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "# https://pytorch.org/vision/main/auto_examples/plot_visualization_utils.html#sphx-glr-auto-examples-plot-visualization-utils-py\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "#show(img_grid)\n",
    "#matplotlib_imshow(img_grid, one_channel=False)\n",
    "#print('  '.join(classes[labels[j]] for j in range(4))\n",
    "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
    "# torch.utils.tensorboard.SummaryWriter is imported above\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/visualise_data')\n",
    "\n",
    "# Write image data to TensorBoard log dir\n",
    "writer.add_image('Sat Images', img_grid)\n",
    "writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#  tensorboard --logdir=runs\n",
    "# ...and open a browser tab to http://localhost:6006/\n",
    "\n",
    "#if on remote\n",
    "## on remote\n",
    "##tensorboard --logdir <path> --port 6006\n",
    "## forward everything on port 6006 on server to \n",
    "##ssh -L 16006:127.0.0.1:6006 -i vmexjobb_key.pem exjobbare@vdexjobb.westeurope.cloudapp.azure.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a5e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/oskar/ESSD/deepSat/model/models.py:56: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape != concat_layer.shape:\n"
     ]
    }
   ],
   "source": [
    "#visualizing model in tensorboard\n",
    "\n",
    "\n",
    "# Again, grab a single mini-batch of images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# add_graph() will trace the sample input through your model,\n",
    "# and render it as a graph.\n",
    "writer.add_graph(model, images)\n",
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a772cf3982d27f7efa5dd5ee7ce70eb2de69f36da9772dbcefc10fee67f331e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deepsat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
